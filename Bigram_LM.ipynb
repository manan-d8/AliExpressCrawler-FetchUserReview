{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Bigram_LM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3xRmdeXSE3eX",
        "2PfddoybIYFa",
        "LEv3fjcz_iZi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDVXWWQDyfHu",
        "colab_type": "text"
      },
      "source": [
        "### Import required libraries including NLTK and Reuters Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO887GAc2IlP",
        "colab_type": "code",
        "outputId": "11e5ad9d-9c34-419a-b796-6ae055b26e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters') # one time execution\n",
        "nltk.download('punkt')  # one time execution\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWGjl_py5yP5",
        "colab_type": "text"
      },
      "source": [
        "### Know the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnS_OL2bEvw",
        "colab_type": "code",
        "outputId": "071ee503-3a58-4d51-9e2f-08ec1f10f1ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# see the data\n",
        "reuters.raw()[:200]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reachin\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4e_ZpBy00z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get sentences\n",
        "dataset_sents = reuters.sents()\n",
        "# print first sentence\n",
        "dataset_sents[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q63sKCJxy9gc",
        "colab_type": "code",
        "outputId": "d2ad790f-205d-4c94-efa1-0847d59542c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# number of sentences\n",
        "len(dataset_sents)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SgWoXcXzIcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3de7fb8f-6523-4991-9f66-2207c0eb866e"
      },
      "source": [
        "# get list of words\n",
        "dataset_words = reuters.words()\n",
        "# print first word\n",
        "dataset_words[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24IEtnUAzeCc",
        "colab_type": "code",
        "outputId": "be95dcfe-4091-4b28-ee79-9f37c54fc7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# total number of words\n",
        "len(dataset_words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1720901"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00Do0vfczlOb",
        "colab_type": "code",
        "outputId": "4bc7f5c8-92e0-42a1-ade0-01b2e6aa6555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# size of vocabulary\n",
        "len(set(dataset_words))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnXiiFQ759qH",
        "colab_type": "text"
      },
      "source": [
        "### Split the Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reMY08314XMK",
        "colab_type": "text"
      },
      "source": [
        "Divide the dataset into two Train data and Test data. Bigram Probabilities are learnt on the Train data and to evaluate it, we use the Test data. <br>\n",
        "Out of 54716 sentences, 40000 sentences are used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUiQqArqkwEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_sents = dataset_sents[:40000]\n",
        "data_sents_test = dataset_sents[40000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g65ozpPV5JSC",
        "colab_type": "text"
      },
      "source": [
        "Get lists of tokens of train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxKG3jG_0dsD",
        "colab_type": "code",
        "outputId": "810138c2-affe-4b90-e7e7-fc541b9b6232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# number of words in train data\n",
        "num_words = 0\n",
        "for sentence in data_sents:\n",
        "  num_words += len(sentence)\n",
        "num_words"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1262448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-rz7ZIO0iAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create two lists containing words\n",
        "data_words_train = dataset_words[:num_words]\n",
        "data_words_test = dataset_words[num_words:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcSunRes6I8e",
        "colab_type": "text"
      },
      "source": [
        "### Learn Probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbRsLiH6sk-",
        "colab_type": "text"
      },
      "source": [
        "Method to generate a list of bigrams, and to get frequencies of unigrams and bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vizo7IM-Mhxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createBigram(data):\n",
        "\tlistOfBigrams = []\n",
        "\tbigramCounts = {}\n",
        "\tunigramCounts = {}\n",
        "\n",
        "\tfor i in range(len(data)):\n",
        "\t\tif i < len(data) - 1:\n",
        "\n",
        "\t\t\tlistOfBigrams.append((data[i], data[i + 1]))\n",
        "\n",
        "\t\t\tif (data[i], data[i+1]) in bigramCounts:\n",
        "\t\t\t\tbigramCounts[(data[i], data[i + 1])] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tbigramCounts[(data[i], data[i + 1])] = 1\n",
        "\n",
        "\t\tif data[i] in unigramCounts:\n",
        "\t\t\tunigramCounts[data[i]] += 1\n",
        "\t\telse:\n",
        "\t\t\tunigramCounts[data[i]] = 1\n",
        "\n",
        "\treturn listOfBigrams, unigramCounts, bigramCounts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z87ncOlM7Qm-",
        "colab_type": "text"
      },
      "source": [
        "Method to learn bigram probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdMfz56Al2LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
        "\n",
        "\tlistOfProb = {}\n",
        "\tfor bigram in listOfBigrams:\n",
        "\t\tword1 = bigram[0]\n",
        "\t\tword2 = bigram[1]\n",
        "\t\t\n",
        "\t\tlistOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
        "\n",
        "\tfile = open('bigramProb.txt', 'w')\n",
        "\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
        "\n",
        "\tfor bigrams in listOfBigrams:\n",
        "\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams]) + ' : ' + str(listOfProb[bigrams]) + '\\n')\n",
        "\tfile.close()\n",
        "\n",
        "\treturn listOfProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25qCUF0N7YHd",
        "colab_type": "text"
      },
      "source": [
        "Method to learn bigram probabilities with Add-1 Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9pe7LMO5exi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts):\n",
        "\n",
        "\tlistOfProb = {}\n",
        "\tcStar = {}\n",
        "\n",
        "\n",
        "\tfor bigram in listOfBigrams:\n",
        "\t\tword1 = bigram[0]\n",
        "\t\tword2 = bigram[1]\n",
        "\t\tlistOfProb[bigram] = (bigramCounts.get(bigram) + 1)/(unigramCounts.get(word1) + len(unigramCounts))\n",
        "\t\tcStar[bigram] = (bigramCounts[bigram] + 1) * unigramCounts[word1] / (unigramCounts[word1] + len(unigramCounts))\n",
        "\n",
        "\tfile = open('addOneSmoothing.txt', 'w')\n",
        "\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
        "\n",
        "\tfor bigrams in listOfBigrams:\n",
        "\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams])\n",
        "\t\t\t\t   + ' : ' + str(listOfProb[bigrams]) + '\\n')\n",
        "\n",
        "\tfile.close()\n",
        "\n",
        "\treturn listOfProb, cStar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUqKF63arb6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main Program\n",
        "\n",
        "# Create a list of bigrams and get frequencies of unigrams and bigrams\n",
        "listOfBigrams, unigramCounts, bigramCounts = createBigram(data_words_train)\n",
        "\n",
        "# Calculate bigram probabilities\n",
        "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
        "\n",
        "# Apply Add-1 Smoothing and calculate probabilities and get reconstructed count of bigrams\n",
        "bigramAddOne, addOneCstar = bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ll7utCF6QRX",
        "colab_type": "text"
      },
      "source": [
        "### Count Sentence Probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY5gGr1D-78E",
        "colab_type": "text"
      },
      "source": [
        "Input a sentence and generate bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGd4pxsOhiV7",
        "colab_type": "code",
        "outputId": "0fe9f797-313b-4276-965c-4243cd7f3492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "input = 'we must be very careful'\n",
        "\n",
        "inputList = [] # list to store bigrams\n",
        "\n",
        "for i in range(len(input.split())-1):\n",
        "  inputList.append((input.split()[i], input.split()[i+1]))\n",
        "inputList"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uIyvjaW_c2q",
        "colab_type": "text"
      },
      "source": [
        "Get Bigram counts and probabilities use them to calculate probability of the input sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD5UlEA29PP0",
        "colab_type": "code",
        "outputId": "0b557994-6873-45c4-b3c5-de7c5a1cd5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Open a file to write output\n",
        "output1 = open('bigramProb-OUTPUT.txt', 'w')\n",
        "\n",
        "# initial probability of a sentence\n",
        "outputProb1 = 1\n",
        "\n",
        "output1.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n",
        "\n",
        "for i in range(len(inputList)):\n",
        "\n",
        "  # if bigram is present in the model, get updated probability\n",
        "  if inputList[i] in bigramProb:\n",
        "    # write bigram, its count and probability to the file\n",
        "    output1.write(str(inputList[i]) + '\\t\\t' + str(bigramCounts[inputList[i]]) + '\\t\\t' + str(bigramProb[inputList[i]]) + '\\n')\n",
        "    # multiply with probability of a current bigram\n",
        "    outputProb1 *= bigramProb[inputList[i]]\n",
        "\n",
        "  # if bigram is not present in the model, sentence probability is zero\n",
        "  else:\n",
        "    output1.write(str(inputList[i]) + '\\t\\t\\t' + str(0) + '\\t\\t\\t' + str(0) + '\\n')\n",
        "    outputProb1 *= 0\n",
        "\n",
        "output1.write('\\n' + 'Probablility = ' + str(outputProb1))\n",
        "outputProb1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.8076159793430567e-07"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btRgkXbDEOov",
        "colab_type": "text"
      },
      "source": [
        "Use Add-1 Smoothed model to get the probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmrxjZ899-tz",
        "colab_type": "code",
        "outputId": "acbf82c5-4b1c-4f71-90b3-7b71fe3ee216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Open a file to write output\n",
        "output2 = open('addOneSmoothing-OUTPUT.txt', 'w')\n",
        "\n",
        "# initial probability of a sentence\n",
        "outputProb2 = 1\n",
        "\n",
        "output2.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n",
        "\n",
        "for i in range(len(inputList)):\n",
        "\n",
        "  # if bigram is present in the model, get updated probability\n",
        "  if inputList[i] in bigramAddOne:\n",
        "    # Update probability of the sentence\n",
        "    outputProb2 *= bigramAddOne[inputList[i]]\n",
        "\n",
        "    output2.write(str(inputList[i]) + '\\t\\t' + str(addOneCstar[inputList[i]]) + '\\t\\t' + str(bigramAddOne[inputList[i]]) + '\\n')\n",
        "\n",
        "  # if bigram is not present in the model, use unigram counts to get estimated probability\n",
        "  else:\n",
        "    # if first word in a bigram is not present in unigrams, add with with count 1\n",
        "    if inputList[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList[i][0]] = 1\n",
        "    \n",
        "    # calculate probability of that word\n",
        "    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "\n",
        "    # # reconstructed count for the bigram\n",
        "    addOneCStar = 1 * unigramCounts[inputList[i][0]] / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "    \n",
        "    # Update probability of the sentence\n",
        "    outputProb2 *= prob\n",
        "\n",
        "    output2.write(str(inputList[i]) + '\\t' + str(addOneCStar) + '\\t' + str(prob) + '\\n')\n",
        "\n",
        "output2.write('\\n' + 'Probablility = ' + str(outputProb2))\n",
        "outputProb2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2254340301928457e-14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSSQl6WzEZw9",
        "colab_type": "text"
      },
      "source": [
        "Show the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKLRvTgC-DeC",
        "colab_type": "code",
        "outputId": "fb71dcac-1c29-4293-b569-d28fbf3a74bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# input sentence\n",
        "print(input)\n",
        "\n",
        "# list of bigrams\n",
        "print(inputList)\n",
        "\n",
        "# probability given by simple bigram model\n",
        "print ('Bigram Model: ', outputProb1)\n",
        "\n",
        "# probability given by bigram model with add-1 smoothing\n",
        "print ('Add One: ', outputProb2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we must be very careful\n",
            "[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]\n",
            "Bigram Model:  2.8076159793430567e-07\n",
            "Add One:  4.2254340301928457e-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnoVrDtQKSBX",
        "colab_type": "text"
      },
      "source": [
        "### Next Word Recommandation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulm4Oo55ZH3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_prob_with_next_word(next_word):\n",
        "  outputProb = 1\n",
        "  new_bigram = (input.split()[-1], next_word)\n",
        "  if new_bigram in bigramAddOne:\n",
        "    outputProb *= bigramAddOne[new_bigram]\n",
        "  else:\n",
        "    if new_bigram[0] not in unigramCounts:\n",
        "      unigramCounts[new_bigram[0]] = 1\n",
        "    prob = (1) / (unigramCounts[new_bigram[0]] + len(unigramCounts))\n",
        "    outputProb *= prob\n",
        "  return outputProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI-D6IgAR7L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = 'the investors are'\n",
        "possible_words = ['cheated', 'happy', 'smart', 'afraid']\n",
        "\n",
        "inputList = []\n",
        "outputProb = 1\n",
        "\n",
        "for i in range(len(input.split())-1):\n",
        "  inputList.append((input.split()[i], input.split()[i+1]))\n",
        "\n",
        "\n",
        "for i in range(len(inputList)):\n",
        "\n",
        "  if inputList[i] in bigramAddOne:\n",
        "    outputProb *= bigramAddOne[inputList[i]]\n",
        "  else:\n",
        "    if inputList[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList[i][0]] = 1\n",
        "    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "    outputProb *= prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQP7k6ZhKlZw",
        "colab_type": "code",
        "outputId": "53947bec-d7f1-43d5-a368-6bd2f4bea899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "max_prob = 0\n",
        "index_of_next_word = -1\n",
        "for i, word in enumerate(possible_words):\n",
        "  final_prob = outputProb * sentence_prob_with_next_word(word)\n",
        "  if final_prob > max_prob:\n",
        "    max_prob = final_prob\n",
        "    index_of_next_word = i\n",
        "\n",
        "print('Next Word:', possible_words[index_of_next_word])\n",
        "print('Output Sentece:', input, possible_words[index_of_next_word])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next Word: happy\n",
            "Output Sentece: the investors are happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvpm9Wnv-auQ",
        "colab_type": "text"
      },
      "source": [
        "### Word Order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxHSAzEx-ekP",
        "colab_type": "code",
        "outputId": "ae1c2ee6-6714-4c41-ceb9-47b8b2dce7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "input1 = 'the market is very happy these days'\n",
        "input2 = 'market is the happy these very days'\n",
        "\n",
        "\n",
        "inputList1 = []\n",
        "inputList2 = []\n",
        "\n",
        "\n",
        "outputProb1 = 1\n",
        "outputProb2 = 1\n",
        "\n",
        "\n",
        "for i in range(len(input1.split())-1):\n",
        "  inputList1.append((input1.split()[i], input1.split()[i+1]))\n",
        "\n",
        "for i in range(len(input2.split())-1):\n",
        "  inputList2.append((input2.split()[i], input2.split()[i+1]))\n",
        "\n",
        "\n",
        "for i in range(len(inputList1)):\n",
        "  if inputList1[i] in bigramAddOne:\n",
        "    outputProb1 *= bigramAddOne[inputList1[i]]\n",
        "  else:\n",
        "    if inputList1[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList1[i][0]] = 1\n",
        "    prob1 = (1) / (unigramCounts[inputList1[i][0]] + len(unigramCounts))\n",
        "    outputProb1 *= prob1\n",
        "\n",
        "\n",
        "for i in range(len(inputList2)):\n",
        "  if inputList2[i] in bigramAddOne:\n",
        "    outputProb2 *= bigramAddOne[inputList2[i]]\n",
        "  else:\n",
        "    if inputList2[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList2[i][0]] = 1\n",
        "    prob2 = (1) / (unigramCounts[inputList2[i][0]] + len(unigramCounts))\n",
        "    outputProb2 *= prob2\n",
        "\n",
        "print (input1, ':', outputProb1)\n",
        "print (input2, ':', outputProb2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the market is very happy these days : 3.0787233025784113e-22\n",
            "market is the happy these very days : 2.5840603259051406e-24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xRmdeXSE3eX",
        "colab_type": "text"
      },
      "source": [
        "### Sentence with Homonyms: piece or peace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NEjoIYOGA61",
        "colab_type": "text"
      },
      "source": [
        "Bigram model with add-1 smoothing is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vav3BVn3F8w_",
        "colab_type": "code",
        "outputId": "261a66e1-190c-4183-a051-5d3db6041167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "input1 = 'its a peace of information'\n",
        "input2 = 'its a piece of information'\n",
        "\n",
        "\n",
        "inputList1 = []\n",
        "inputList2 = []\n",
        "\n",
        "\n",
        "outputProb1 = 1\n",
        "outputProb2 = 1\n",
        "\n",
        "\n",
        "for i in range(len(input1.split())-1):\n",
        "  inputList1.append((input1.split()[i], input1.split()[i+1]))\n",
        "\n",
        "for i in range(len(input2.split())-1):\n",
        "  inputList2.append((input2.split()[i], input2.split()[i+1]))\n",
        "\n",
        "\n",
        "for i in range(len(inputList1)):\n",
        "  if inputList1[i] in bigramAddOne:\n",
        "    outputProb1 *= bigramAddOne[inputList1[i]]\n",
        "  else:\n",
        "    if inputList1[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList1[i][0]] = 1\n",
        "    prob1 = (1) / (unigramCounts[inputList1[i][0]] + len(unigramCounts))\n",
        "    outputProb1 *= prob1\n",
        "\n",
        "\n",
        "for i in range(len(inputList2)):\n",
        "  if inputList2[i] in bigramAddOne:\n",
        "    outputProb2 *= bigramAddOne[inputList2[i]]\n",
        "  else:\n",
        "    if inputList2[i][0] not in unigramCounts:\n",
        "      unigramCounts[inputList2[i][0]] = 1\n",
        "    prob2 = (1) / (unigramCounts[inputList2[i][0]] + len(unigramCounts))\n",
        "    outputProb2 *= prob2\n",
        "\n",
        "print (input1, ':', outputProb1)\n",
        "print (input2, ':', outputProb2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "its a peace of information : 8.014242631151717e-19\n",
            "its a piece of information : 3.2057856418889864e-18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PfddoybIYFa",
        "colab_type": "text"
      },
      "source": [
        "### Perplexity of Model <br>\n",
        "Perplexity of add-1 smoothed bigram model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmxsw-fwIg5Y",
        "colab_type": "text"
      },
      "source": [
        "Method to get probability of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBac7Q0Sg7pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_bigram_sentence_probability(input):\n",
        "\n",
        "  inputList = []\n",
        "  outputProb = 1\n",
        "\n",
        "  for i in range(len(input)-1):\n",
        "    inputList.append((input[i], input[i+1]))\n",
        "\n",
        "  for i in range(len(inputList)):\n",
        "    if inputList[i] in bigramAddOne:\n",
        "      outputProb *= bigramAddOne[inputList[i]]\n",
        "    else:\n",
        "      if inputList[i][0] not in unigramCounts:\n",
        "        unigramCounts[inputList[i][0]] = 1\n",
        "      prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n",
        "      outputProb *= prob\n",
        "\n",
        "  return outputProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8fv-8-kJLHB",
        "colab_type": "text"
      },
      "source": [
        "Method to get total number of bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9i2rXyhQiU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_number_of_bigrams(sentences):\n",
        "        bigram_count = 0\n",
        "        for sentence in sentences:\n",
        "            # remove one for number of bigrams in sentence\n",
        "            bigram_count += len(sentence) - 1\n",
        "        return bigram_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAlU--jrJVna",
        "colab_type": "text"
      },
      "source": [
        "Method to calculate perplexity of a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoUq8FC7HtT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_bigram_perplexity(model, sentences):\n",
        "    number_of_bigrams = calculate_number_of_bigrams(sentences)\n",
        "    bigram_sentence_probability_log_sum = 0\n",
        "    for sentence in sentences:\n",
        "        p = calculate_bigram_sentence_probability(sentence)\n",
        "        if p != 0.0:\n",
        "          a = math.log(p)\n",
        "        else:\n",
        "          a = 0\n",
        "        bigram_sentence_probability_log_sum -= a\n",
        "    return math.pow(2, bigram_sentence_probability_log_sum / number_of_bigrams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmwFEZIhJu4Q",
        "colab_type": "text"
      },
      "source": [
        "Perplexity of the model over train and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydUwPXUMNQ6y",
        "colab_type": "code",
        "outputId": "432aeebf-8c46-4c63-c090-b53550f7f3c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(\"PERPLEXITY over Training Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents))\n",
        "print(\"PERPLEXITY over Test Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents_test))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERPLEXITY over Training Data: 137.07939217258775\n",
            "PERPLEXITY over Test Data: 168.93416630740222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7iZZDMLwIGN",
        "colab_type": "text"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCsjMMDu4z9v",
        "colab_type": "text"
      },
      "source": [
        "Text Generation Using Simple Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm88ndODxEsU",
        "colab_type": "code",
        "outputId": "867afe69-6bc7-485a-8a35-fda8ce3fa6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# initial word\n",
        "text = [\"there\"]\n",
        "\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = 0.0\n",
        "\n",
        "  for pair in bigramProb.keys():\n",
        "    if pair[0] == text[-1]:\n",
        "      accumulator += bigramProb[pair]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(pair[1])\n",
        "          break\n",
        "\n",
        "  if text[-1] == 'None':\n",
        "    sentence_finished = True\n",
        "  if len(text) > 20:\n",
        "    sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text if t]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there is expected within Europe and a free areas in January , 470 tonnes , 682 Avg shrs 30 Oper net\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7yUBX2Db48aH"
      },
      "source": [
        "Text Generation Using Simple Trigram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhw3fGeX2cvA",
        "colab_type": "code",
        "outputId": "94ad1c0f-1720-4f0d-d886-7a32fc4ccc87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "# initial words\n",
        "text = [\"there\", \"is\"]\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = 0.0\n",
        "\n",
        "  for tpl in trigramProb.keys():\n",
        "    if (tpl[0] == text[-2] and tpl[1] == text[-1]):\n",
        "      accumulator += trigramProb[tpl]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(tpl[2])\n",
        "          break\n",
        "\n",
        "  if text[-2:] == ['None', 'None']:\n",
        "    sentence_finished = True\n",
        "  if len(text) > 20:\n",
        "    sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text if t]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-51f0e340f8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mtpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrigramProb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0maccumulator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrigramProb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtpl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trigramProb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR1NU8oY7jiZ",
        "colab_type": "text"
      },
      "source": [
        "###Assignment #1 Create A Trigram model based on the code given for the Bigram with Add-1 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-9Z35K771hQ",
        "colab_type": "text"
      },
      "source": [
        "###Assignment #2 Compute the Perplexity of the created Triagram Model and compare it with Bigram model as given above"
      ]
    }
  ]
}